{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install openslide-tools\n",
        "# !pip install openslide-python\n",
        "%matplotlib inline\n",
        "from openslide import open_slide, __library_version__ as openslide_version\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2gray\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision.transforms.functional import hflip, vflip, rotate, adjust_hue, adjust_contrast, adjust_brightness, adjust_saturation\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "0QGptwpITfQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key Parameters \n",
        "download_data = 0 # download the slide image and masks from google bucket\n",
        "gen_data = 0 # generate the np array data for our model\n",
        "\n",
        "ngf = 8 # number of channels that the generator starts with\n",
        "ndf = 8 # number of channels that the discriminator starts with\n",
        "\n",
        "batch = 4 # batch size of a dataset\n",
        "nc_i = 9 # number of channels in the image (3 images, each 3 channels)\n",
        "nc_m = 1 # number of channels in the mask\n",
        "height = 128 # height of mask/image\n",
        "width = 128 # width of mask/image"
      ],
      "metadata": {
        "id": "zGWR3OpLblK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "idL6MTglbCDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the image the google bucket that I set up \n",
        "# credit: datasset is by Joshua Gordon for the COMS 4995 Applied Deep Leaning in Fall 2022\n",
        "if download_data == 1: \n",
        "    image_url = 'https://storage.googleapis.com/acv_project/adl_slides.zip'\n",
        "    !curl -O $image_url\n",
        "    !unzip adl_slides\n",
        "    !rm adl_slides.zip"
      ],
      "metadata": {
        "id": "fA9mT4-uTmtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_slide(slide, x, y, level, width, height, as_float=False):\n",
        "    im = slide.read_region((x,y), level, (width, height))\n",
        "    im = im.convert('RGB') # drop the alpha channel\n",
        "    if as_float:\n",
        "        im = np.asarray(im, dtype=np.float32)\n",
        "    else:\n",
        "        im = np.asarray(im)\n",
        "    assert im.shape == (height, width, 3)\n",
        "    return im"
      ],
      "metadata": {
        "id": "MPIcCt5aV8BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = 250\n",
        "col = 400\n",
        "slide = open_slide(\"./acv_slides/tumor_075.tif\")\n",
        "mask = open_slide(\"./acv_slides/tumor_075_mask.tif\")\n",
        "\n",
        "# notice that the tumor is very small and it can only be viewed at a level 0 to level 4 region\n",
        "lev_7 = read_slide(slide, \n",
        "                x = 0, \n",
        "                y = 0, \n",
        "                level = 7, \n",
        "                width = slide.level_dimensions[7][0], \n",
        "                height = slide.level_dimensions[7][1])\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_7)\n",
        "\n",
        "lev_7_mask = read_slide(mask, \n",
        "                x = 0, \n",
        "                y = 0, \n",
        "                level = 7, \n",
        "                width = slide.level_dimensions[7][0], \n",
        "                height = slide.level_dimensions[7][1])\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_7_mask[:,:,0])\n",
        "\n",
        "# note the way we can alter the zoom levels\n",
        "# we keep the center point fixed and expand the window around it\n",
        "lev_4 = read_slide(slide, \n",
        "                    x = (row-1)*128 - 64*7, \n",
        "                    y = (col-1)*128 - 64*7, \n",
        "                    level = 4, \n",
        "                    width = 128, \n",
        "                    height = 128)\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_4)\n",
        "\n",
        "lev_4_mask = read_slide(mask, \n",
        "                        x = (row-1)*128 - 64*7, \n",
        "                        y = (col-1)*128 - 64*7, \n",
        "                        level = 4, \n",
        "                        width = 128, \n",
        "                        height = 128)\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_4_mask[:,:,0])\n",
        "\n",
        "lev_2 = read_slide(slide, \n",
        "                    x = (row-1)*128 - 64*3, \n",
        "                    y = (col-1)*128 - 64*3, \n",
        "                    level = 2, \n",
        "                    width = 128, \n",
        "                    height = 128)\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_2)\n",
        "\n",
        "lev_2_mask = read_slide(mask, \n",
        "                        x = (row-1)*128 - 64*3, \n",
        "                        y = (col-1)*128 - 64*3, \n",
        "                        level = 2, \n",
        "                        width = 128, \n",
        "                        height = 128)\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_2_mask[:,:,0])\n",
        "\n",
        "lev_0 = read_slide(slide, \n",
        "                    x = (row-1)*128, \n",
        "                    y = (col-1)*128, \n",
        "                    level = 0, \n",
        "                    width = 128, \n",
        "                    height = 128)\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_0)\n",
        "\n",
        "\n",
        "lev_0_mask = read_slide(mask, \n",
        "                        x = (row-1)*128, \n",
        "                        y = (col-1)*128, \n",
        "                        level = 0, \n",
        "                        width = 128, \n",
        "                        height = 128)\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.imshow(lev_0_mask[:,:,0])"
      ],
      "metadata": {
        "id": "vHUK-Mp9pppN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some simple helper functions\n",
        "\n",
        "def find_tissue_pixels(image, intensity=0.8):\n",
        "    im_gray = rgb2gray(image)\n",
        "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
        "    indices = np.where(im_gray <= intensity)\n",
        "    return list(zip(indices[0], indices[1]))\n",
        "\n",
        "def have_cell(image, mask, intensity=0.8, percentage_thres = 0.15): # originally at 5%\n",
        "    if np.sum(mask)>0:\n",
        "        return True\n",
        "    im_gray = rgb2gray(image)\n",
        "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
        "    cell_percentage = np.mean(im_gray <= intensity)\n",
        "    if cell_percentage > percentage_thres:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "h_rPK1nNt1xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(slide_id, output_path, root_path = './acv_slides', perc_neg = 0.8, cap = 500): \n",
        "    '''\n",
        "    create the data for the model to train and evaluate on\n",
        "\n",
        "    slide_id: the id of the slide that we will be working on\n",
        "    output_path: where to save the output\n",
        "    root_path: where the slide will be resciding in\n",
        "    perc_neg: the approximate proportion of samples that will have no tumor at all \n",
        "    cap: the maximum number of samples from this slide\n",
        "    '''\n",
        "\n",
        "    image_path = os.path.join(root_path, slide_id + '.tif')\n",
        "    mask_path = os.path.join(root_path, slide_id + '_mask.tif')\n",
        "    \n",
        "    # create output file\n",
        "    if not os.path.exists(output_path):\n",
        "        os.mkdir(output_path) # save all data to this path\n",
        "\n",
        "    slide = open_slide(image_path)\n",
        "    tumor_mask = open_slide(mask_path)\n",
        "    dim_x, dim_y = slide.level_dimensions[7]\n",
        "\n",
        "    slide_image = read_slide(slide, \n",
        "                             x=0, \n",
        "                             y=0, \n",
        "                             level=7, \n",
        "                             width=dim_x, \n",
        "                             height=dim_y)\n",
        "\n",
        "    mask_image = read_slide(tumor_mask, \n",
        "                            x=0, \n",
        "                            y=0, \n",
        "                            level=7, \n",
        "                            width=dim_x, \n",
        "                            height=dim_y)\n",
        "\n",
        "    indices = find_tissue_pixels(slide_image)\n",
        "    random.shuffle(indices) # shuffle it so that every indice is equally likely to appear as a sample\n",
        "\n",
        "    # compute the number of positive cells\n",
        "    perc_pos = np.sum(mask_image[:,:,0])/len(indices)\n",
        "\n",
        "    id = 0\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "\n",
        "    for row, col in indices:\n",
        "        # save image if it is a positive tumor in the middle and randomly select 80 other images\n",
        "        c = np.random.uniform()\n",
        "        if (mask_image[row][col][0] == 1) or (c <= perc_pos * perc_neg):\n",
        "\n",
        "            # count the number of slides added\n",
        "            id += 1\n",
        "            if mask_image[row][col][0] == 1:\n",
        "                pos_count += 1\n",
        "            else: \n",
        "                neg_count += 1\n",
        "\n",
        "            lev_2_slide = read_slide(slide, \n",
        "                                    x = col*128 - 64*3, \n",
        "                                    y = row*128 - 64*3, \n",
        "                                    level = 2, width = 128, height = 128)\n",
        "            \n",
        "            lev_3_slide = read_slide(slide, \n",
        "                                    x = col*128 - 64*5, \n",
        "                                    y = row*128 - 64*5, \n",
        "                                    level = 3, width = 128, height = 128)\n",
        "\n",
        "            lev_4_slide = read_slide(slide, \n",
        "                                    x = col*128 - 64*7, \n",
        "                                    y = row*128 - 64*7, \n",
        "                                    level = 4, width = 128, height = 128)\n",
        "            \n",
        "            mask = read_slide(tumor_mask, \n",
        "                            x = col*128 - 64*5, \n",
        "                            y = row*128 - 64*5, \n",
        "                            level = 3, width = 128, height = 128)            \n",
        "            np.save(os.path.join(output_path, slide_id + str(id)), np.array([lev_2_slide, lev_3_slide, lev_4_slide, mask]))\n",
        "            \n",
        "            # prevent the overdominance of a single slide\n",
        "            if pos_count > cap: \n",
        "                break\n",
        "\n",
        "    print(slide_id + \": {} positive slides and {} negative slides added to '{}'\".format(int(pos_count), int(neg_count), output_path))\n",
        "    return None"
      ],
      "metadata": {
        "id": "dLhgRdlvu3mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the 3 layers of the slides and the mask image\n",
        "\n",
        "if gen_data == 1: \n",
        "    model_path = './data'\n",
        "    if not os.path.exists(model_path):\n",
        "        os.mkdir(model_path) # save all models to this path\n",
        "    else: \n",
        "        !rm -rf './data/'\n",
        "        os.mkdir(model_path)\n",
        "\n",
        "    train_slides_names = [\n",
        "                          'tumor_001',\n",
        "                          'tumor_002',\n",
        "                          'tumor_005',\n",
        "                          'tumor_012',\n",
        "                          'tumor_016',\n",
        "                          'tumor_031',\n",
        "                          'tumor_035',\n",
        "                          'tumor_059',\n",
        "                          'tumor_064',\n",
        "                          'tumor_075',\n",
        "                          'tumor_078',\n",
        "                          'tumor_081',\n",
        "                          'tumor_084',\n",
        "                          'tumor_091',\n",
        "                          'tumor_096',\n",
        "                          'tumor_110']\n",
        "\n",
        "    for slide_id in train_slides_names:\n",
        "        generate_data(slide_id, output_path = './data/train')\n",
        "\n",
        "    valid_slides_names = ['tumor_057',\n",
        "                          'tumor_019']\n",
        "\n",
        "    for slide_id in valid_slides_names:\n",
        "        generate_data(slide_id, output_path = './data/val')"
      ],
      "metadata": {
        "id": "Hmb3Z70ozY7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader (adapted from the lecture slides)\n",
        "\n",
        "class MyImageDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        There are only 2 files, one for training and one for validation\n",
        "        The tumor mask is stored along with the cell image\n",
        "    \"\"\"\n",
        "    def __init__(self, images_dir, image_transform = True):\n",
        "\n",
        "        self.images_dir = images_dir\n",
        "        # note that only flip and rotation applies to tumor mask\n",
        "        self.image_transform = image_transform\n",
        "        \n",
        "        # Next, let's collect all image files underneath each class name directory as a single list of image files. \n",
        "        # note that we use a np array to prevent memory leak\n",
        "        self.image_files = [os.path.join(images_dir, img_id) for img_id in os.listdir(images_dir)]\n",
        "     \n",
        "        # How many total images do we need to iterate in this entire dataset?\n",
        "        self.num_images = len(self.image_files)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.num_images\n",
        "    \n",
        "    def __getitem__(self, idx):  \n",
        "        # Retrieve the images from the list, load it, transform it, \n",
        "        # concat the transformed images together along the channel axis\n",
        "        # and return it along with its ground truth label.  \n",
        "        # need the image and mask to be in tensor form \n",
        "\n",
        "        sample = np.load(self.image_files[idx])\n",
        "        lev_2_image = Image.fromarray(sample[0])\n",
        "        lev_3_image = Image.fromarray(sample[1])\n",
        "        lev_4_image = Image.fromarray(sample[2])\n",
        "        mask = Image.fromarray(sample[3])\n",
        "        \n",
        "        # Apply the image transformations if needed\n",
        "        if self.image_transform: \n",
        "            # flipping\n",
        "            c = np.random.randint(0,3)\n",
        "            if c == 1: \n",
        "                lev_2_image = hflip(lev_2_image)\n",
        "                lev_3_image = hflip(lev_3_image)\n",
        "                lev_4_image = hflip(lev_4_image)\n",
        "                mask = hflip(mask)\n",
        "            elif c == 2: \n",
        "                lev_2_image = vflip(lev_2_image)\n",
        "                lev_3_image = vflip(lev_3_image)\n",
        "                lev_4_image = vflip(lev_4_image)\n",
        "                mask = vflip(mask)\n",
        "            elif c == 3: \n",
        "                lev_2_image = hflip(lev_2_image)\n",
        "                lev_3_image = hflip(lev_3_image)\n",
        "                lev_4_image = hflip(lev_4_image)\n",
        "                mask = hflip(mask)\n",
        "                lev_2_image = vflip(lev_2_image)\n",
        "                lev_3_image = vflip(lev_3_image)\n",
        "                lev_4_image = vflip(lev_4_image)\n",
        "                mask = vflip(mask)\n",
        "\n",
        "            # rotation\n",
        "            c = np.random.randint(0,3)\n",
        "            lev_2_image  = rotate(lev_2_image, 90*c)\n",
        "            lev_3_image  = rotate(lev_3_image, 90*c)\n",
        "            lev_4_image  = rotate(lev_4_image, 90*c)\n",
        "            mask = rotate(mask, 90*c)\n",
        "\n",
        "            # color gittering by 50% (only for the image)\n",
        "            # adjust brightness\n",
        "            c = np.random.uniform(0.5,1.5)\n",
        "            lev_2_image  = adjust_brightness(lev_2_image, c)\n",
        "            lev_3_image  = adjust_brightness(lev_3_image, c)\n",
        "            lev_4_image  = adjust_brightness(lev_4_image, c)\n",
        "\n",
        "            # adjust contrast\n",
        "            c = np.random.uniform(0.5,1.5)\n",
        "            lev_2_image  = adjust_contrast(lev_2_image, c)\n",
        "            lev_3_image  = adjust_contrast(lev_3_image, c)\n",
        "            lev_4_image  = adjust_contrast(lev_4_image, c)\n",
        "\n",
        "            # adjust saturation\n",
        "            c = np.random.uniform(0.5,1.5)\n",
        "            lev_2_image  = adjust_saturation(lev_2_image, c)\n",
        "            lev_3_image  = adjust_saturation(lev_3_image, c)\n",
        "            lev_4_image  = adjust_saturation(lev_4_image, c)\n",
        "\n",
        "            # adjust hue\n",
        "            c = np.random.uniform(-0.2,0.2)\n",
        "            lev_2_image  = adjust_hue(lev_2_image, c)\n",
        "            lev_3_image  = adjust_hue(lev_3_image, c)\n",
        "            lev_4_image  = adjust_hue(lev_4_image, c)\n",
        "\n",
        "        lev_2_image = ToTensor()(lev_2_image)\n",
        "        lev_3_image = ToTensor()(lev_3_image)\n",
        "        lev_4_image = ToTensor()(lev_4_image)\n",
        "        image = torch.cat([lev_2_image, lev_3_image, lev_4_image], 0)\n",
        "        mask = ToTensor()(mask)\n",
        "        return image, mask # note the implicit permute here"
      ],
      "metadata": {
        "id": "dKisNECO6qDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Now collate into mini-batches\n",
        "    images = torch.stack([b[0] for b in batch]) \n",
        "    masks = torch.stack([b[1] for b in batch])\n",
        "\n",
        "    return images, masks[:,0:1,:,:]"
      ],
      "metadata": {
        "id": "umgm3qwGYrMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './data'\n",
        "\n",
        "data_transforms = {'train': ['flip','rotation','gitter'],\n",
        "                    'val': []}\n",
        "\n",
        "# implement custom image_dataset and wrap it with the dataloader\n",
        "image_datasets = {x: MyImageDataset(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, \n",
        "                                              shuffle=True, num_workers=0, collate_fn = collate_fn)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
      ],
      "metadata": {
        "id": "W4tgBs8yNGhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net Generator\n",
        "# implementation from https://github.com/usuyama/pytorch-unet\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )   \n",
        "\n",
        "class Generator_Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dconv_down1 = double_conv(nc_i, 64)  \n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512)        \n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, nc_m)\n",
        "        self.conv_last = nn.Sequential(nn.Conv2d(nc_m, 1, 1),\n",
        "                                       nn.Sigmoid())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # input shape = (9, height, width)\n",
        "        # downconv block 1 (output shape = (64, height/2, width/2))\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        # downconv block 2 (output shape = (128, height/4, width/4))\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        # downconv block 3 (output shape = (256, height/8, width/8))\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)   \n",
        "        # upconv layer (output shape = (512, height/4, width/4))\n",
        "        x = self.dconv_down4(x)\n",
        "        x = self.upsample(x)        \n",
        "        # upconv layer (output shape = (256, height/2, width/2))\n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.dconv_up3(x)\n",
        "        x = self.upsample(x)        \n",
        "        # upconv layer (output shape = (128, height, width))\n",
        "        x = torch.cat([x, conv2], dim=1)       \n",
        "        x = self.dconv_up2(x)\n",
        "        x = self.upsample(x)        \n",
        "        # last layer (output shape = (1, height, width))\n",
        "        x = torch.cat([x, conv1], dim=1)   \n",
        "        x = self.dconv_up1(x)\n",
        "        out = self.conv_last(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Y6hWV_Xp0KZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the generator and disriminator network separately\n",
        "# adapted from the model given in https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391#:~:text=GAN%20Training&text=Step%201%20%E2%80%94%20Select%20a%20number,both%20fake%20and%20real%20images.\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    takes in a mask object and determine if it is real or generated\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input shape = (nc_m + nc_i, height, width)\n",
        "            # discriminator block 1 (output shape = (ndf, height/2, width/2))\n",
        "            nn.Conv2d(nc_m + nc_i, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # discriminator block 2 (output shape = (ndf * 2, height/4, width/4))\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # discriminator block 3 (output shape = (ndf * 4, height/8, width/8))\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # discriminator block 4 (output shape = (ndf * 8, height/16, width/16))\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # average and return a binary output\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(4096,1), # ndf*8*height/16*width/16\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        input = torch.cat((image, mask), 1) # merge on channels\n",
        "        output = self.main(input)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Tz6WmmtIDa9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a place to save memory\n",
        "model_path = './model'\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path) # save all models to this path\n",
        "\n",
        "# parameters for training the U-Net\n",
        "model_g = Generator_Unet().to(device)\n",
        "optimizer_g = optim.Adam(model_g.parameters(), lr=0.01) \n",
        "scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size=7, gamma=0.1)\n",
        "criterion_g = nn.BCELoss()\n",
        "num_epochs = 15 # we just want to warm start the generator here"
      ],
      "metadata": {
        "id": "yw5WF9gUdpS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the U-Net Model\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training step\n",
        "    model_g.train()\n",
        "    train_running_loss = 0\n",
        "    for inputs, labels in tqdm(dataloaders['train']):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer_g.zero_grad()\n",
        "        torch.set_grad_enabled(True)\n",
        "        outputs = model_g(inputs)\n",
        "        loss = criterion_g(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_g.step()\n",
        "        train_running_loss += loss.item() * inputs.size(0)\n",
        "    train_loss = train_running_loss/dataset_sizes['train']\n",
        "    train_loss_list.append(train_loss)\n",
        "    scheduler_g.step()\n",
        "\n",
        "    # validation step\n",
        "    model_g.eval()\n",
        "    val_running_loss = 0\n",
        "    for inputs, labels in tqdm(dataloaders['val']):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        torch.set_grad_enabled(False)\n",
        "        outputs = model_g(inputs)\n",
        "        loss = criterion_g(outputs, labels)\n",
        "        val_running_loss += loss.item() * inputs.size(0)\n",
        "    val_loss = val_running_loss/dataset_sizes['val']\n",
        "    val_loss_list.append(val_loss)\n",
        "\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        best_loss = val_loss\n",
        "        torch.save(model_g.state_dict(), './model/u_net')\n",
        "\n",
        "    print(f'epoch: {epoch}/{num_epochs}, Train Loss: {train_loss:.8f}, Val Loss: {val_loss:.8f}')\n",
        "\n",
        "# load best model weight\n",
        "model_g.load_state_dict(torch.load('./model/u_net'))"
      ],
      "metadata": {
        "id": "1ijA5QGNRZXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the validation loss curve\n",
        "plt.plot(val_loss_list)  \n",
        "plt.title('validation loss for u-net')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5yT2BPqPK3A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters for the rest of the GAN model\n",
        "model_d = Discriminator().to(device)\n",
        "optimizer_d = optim.Adam(model_d.parameters(), lr=0.001)\n",
        "criterion_gan = nn.BCELoss()\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "kMieNLixblee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "# idea is from https://medium.com/intel-student-ambassadors/segmentation-using-generative-adversarial-networks-80a161cf33c0\n",
        "\n",
        "discriminator_loss_list = []\n",
        "generator_loss_list = []\n",
        "generator_val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train step\n",
        "    running_loss_d = 0\n",
        "    running_loss_g = 0\n",
        "    model_d.train()\n",
        "    model_g.train()\n",
        "    for inputs, labels in tqdm(dataloaders['train']):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real (log(D(x)))\n",
        "        optimizer_d.zero_grad()\n",
        "        torch.set_grad_enabled(True)\n",
        "        output = model_d(inputs, labels)\n",
        "        loss_d_real = criterion_gan(output, torch.ones(output.shape, device = device))\n",
        "        loss_d_real.backward()\n",
        "        # train with fake (log(1 - D(G(z))))\n",
        "        fake_labels = model_g(inputs)\n",
        "        output = model_d(inputs, fake_labels)\n",
        "        loss_d_fake = criterion_gan(output, torch.zeros(output.shape, device = device))\n",
        "        loss_d_fake.backward()\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        optimizer_d.step()\n",
        "        running_loss_d += loss_d.item()\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_labels = model_g(inputs)\n",
        "        output = model_d(inputs, fake_labels)\n",
        "        # note that fake labels are real for generator cost\n",
        "        loss_g = criterion_gan(output, torch.ones(output.shape, device = device)) \n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "        running_loss_g += loss_g.item()\n",
        "    discriminator_loss = running_loss_d/dataset_sizes['train']\n",
        "    discriminator_loss_list.append(discriminator_loss)\n",
        "    generator_loss = running_loss_g/dataset_sizes['train']\n",
        "    generator_loss_list.append(generator_loss)\n",
        "\n",
        "    # validation step\n",
        "    model_g.eval()\n",
        "    val_running_loss = 0\n",
        "    for inputs, labels in tqdm(dataloaders['val']):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        torch.set_grad_enabled(False)\n",
        "        outputs = model_g(inputs) \n",
        "        loss = criterion_g(outputs, labels)\n",
        "        val_running_loss += loss.item() * inputs.size(0)\n",
        "    val_loss = val_running_loss/dataset_sizes['val']\n",
        "    generator_val_loss_list.append(val_loss)\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        # save the weights\n",
        "        best_loss = val_loss\n",
        "        torch.save(model_g.state_dict(), './model/gan_generator')\n",
        "        torch.save(model_d.state_dict(), './model/gan_discriminator')\n",
        "\n",
        "    # print the progress to determine if the progress has stagnated\n",
        "    print(f'epoch: {epoch}/{num_epochs}, Generator Loss: {generator_loss:.4f}, Discriminator Loss: {discriminator_loss:.4f}, Generator Validation Loss: {val_loss:.8f}')"
      ],
      "metadata": {
        "id": "LfSTBzB3nO3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(generator_val_loss_list)  \n",
        "plt.title('validation loss for u-net trained using GAN')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_bSdElCSWK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions to output the results\n",
        "\n",
        "def slide_to_torch(slide): \n",
        "    ''' \n",
        "    convert the np array slide to a tensor for model\n",
        "    '''\n",
        "    slide = Image.fromarray(slide)\n",
        "    slide = ToTensor()(slide)\n",
        "    return slide.expand(size = (1, -1, -1, -1))\n",
        "\n",
        "def model_predict(model_g, lev_2_slide, lev_3_slide, lev_4_slide, threshold):\n",
        "    '''\n",
        "    outputs the prediction of the model for a single sample\n",
        "    '''\n",
        "    lev_2_slide = slide_to_torch(lev_2_slide)\n",
        "    lev_3_slide = slide_to_torch(lev_3_slide)\n",
        "    lev_4_slide = slide_to_torch(lev_4_slide)    \n",
        "    input = torch.cat([lev_2_slide, lev_3_slide, lev_4_slide],1)\n",
        "    pred = model_g(input.to(device))\n",
        "    if torch.mean(pred).item() >= 0.00125:\n",
        "        print(torch.mean(pred), torch.max(pred))\n",
        "    return 1 if torch.mean(pred) > threshold else 0  # we want to minimize the chances of missing the highlighted tumor region\n",
        "\n",
        "\n",
        "def accuracy(model, slide_id, threshold = 0.1, root_path = './acv_slides'):\n",
        "    '''\n",
        "    output the accuracy on a level 7 scale \n",
        "    threshold: the average mask value for a positive prediction\n",
        "    '''\n",
        "    image_path = os.path.join(root_path, slide_id + '.tif')\n",
        "    mask_path = os.path.join(root_path, slide_id + '_mask.tif')\n",
        "        \n",
        "\n",
        "    slide = open_slide(image_path)\n",
        "    tumor_mask = open_slide(mask_path)\n",
        "\n",
        "    # get the level 7 data\n",
        "    slide_image = read_slide(slide, \n",
        "                                x=0, \n",
        "                                y=0, \n",
        "                                level=7, \n",
        "                                width=slide.level_dimensions[7][0], \n",
        "                                height=slide.level_dimensions[7][1])\n",
        "\n",
        "    mask_image = read_slide(tumor_mask, \n",
        "                            x=0, \n",
        "                            y=0, \n",
        "                            level=7, \n",
        "                            width=slide.level_dimensions[7][0], \n",
        "                            height=slide.level_dimensions[7][1])\n",
        "\n",
        "    indices = find_tissue_pixels(slide_image)\n",
        "    acc = 0\n",
        "\n",
        "    # predict on a level 7 image\n",
        "\n",
        "    dim_x, dim_y = slide.level_dimensions[5]\n",
        "    for row, col in tqdm(indices):\n",
        "            lev_2_slide = read_slide(slide, \n",
        "                                    x = col*128 - 64*3, \n",
        "                                    y = row*128 - 64*3, \n",
        "                                    level = 2, width = 128, height = 128)\n",
        "            \n",
        "            lev_3_slide = read_slide(slide, \n",
        "                                    x = col*128 - 64*5, \n",
        "                                    y = row*128 - 64*5, \n",
        "                                    level = 3, width = 128, height = 128)\n",
        "\n",
        "            lev_4_slide = read_slide(slide, \n",
        "                                    x = col*128 - 64*7, \n",
        "                                    y = row*128 - 64*7, \n",
        "                                    level = 4, width = 128, height = 128)\n",
        "\n",
        "            acc += (model_predict(model_g, lev_2_slide, lev_3_slide, lev_4_slide, threshold) == mask_image[row][col])\n",
        "            \n",
        "    return acc/len(indices)"
      ],
      "metadata": {
        "id": "bnKHU6IRUY4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_slides_names = ['tumor_094',\n",
        "                     'tumor_101']\n",
        "\n",
        "# load trained U-Net\n",
        "model_g.load_state_dict(torch.load('./model/gan_generator'))\n",
        "model_g.eval()\n",
        "model_g.to(device)\n",
        "\n",
        "accuracy(model_g, test_slides_names[0])[0]\n"
      ],
      "metadata": {
        "id": "1hVYE6aR66Oq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}